getwd()
#Headline Count very useful
#Hour very usefu --> turned 0 into 24
#
dftrain = read.csv("C:/Users/Mark Account/Documents/Software/MIT edx/15.071x Analytics Edge/NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
dftest = read.csv("C:/Users/Mark Account/Documents/Software/MIT edx/15.071x Analytics Edge/NYTimesBlogTest.csv", stringsAsFactors=FALSE)
dftrain$SectionName[dftrain$NewsDesk == "TStyle"] <- "Fashion & Style"
dftrain$SubsectionName[dftrain$NewsDesk == "TStyle"] <- "TStyle"
dftrain$SectionName[dftrain$NewsDesk == "Foreign"] <- "World"
dftrain$SectionName[dftrain$NewsDesk == "OpEd"] <- "Opinion"
dftrain$NewsDesk[dftrain$SectionName == "Opinion"] <- "OpEd"
dftrain$SectionName[dftrain$NewsDesk == "OpEd"] <- "Opinion"
dftrain$SectionName[dftrain$NewsDesk == "Styles" & dftrain$SectionName != "U.S."] <- "Fashion & Style"
dftrain$SubsectionName[dftrain$NewsDesk == "Styles" & dftrain$SectionName == "U.S."] <- "Motherlode"
dftrain$SectionName[dftrain$SectionName == ""] <- "Home"
dftrain2 <- dftrain
for(x in 1:nrow(dftrain)) {
if(dftrain[x, 3] == ""){
dftrain2[x, 3] <- dftrain[x, 2]}
}
dftest$SectionName[dftest$NewsDesk == "TStyle"] <- "Fashion & Style"
dftest$SubsectionName[dftest$NewsDesk == "TStyle"] <- "TStyle"
dftest$SectionName[dftest$NewsDesk == "Foreign"] <- "World"
dftest$SectionName[dftest$NewsDesk == "OpEd"] <- "Opinion"
dftest$NewsDesk[dftest$SectionName == "Opinion"] <- "OpEd"
dftest$SectionName[dftest$NewsDesk == "OpEd"] <- "Opinion"
dftest$SectionName[dftest$NewsDesk == "Styles" & dftest$SectionName != "U.S."] <- "Fashion & Style"
dftest$SubsectionName[dftest$NewsDesk == "Styles" & dftest$SectionName == "U.S."] <- "Motherlode"
dftest$SectionName[dftest$SectionName == ""] <- "Home"
dftest2 <- dftest
for(x in 1:nrow(dftest)) {
if(dftest[x, 3] == ""){
dftest2[x, 3] <- dftest[x, 2]}
}
trainPopular <- as.factor(as.numeric(dftrain2$Popular))
dftrain2 <- dftrain2[-9]
df <- rbind(dftrain2, dftest2)
#turn factor variables into factors
df$NewsDesk <- as.factor(df$NewsDesk)
df$SectionName <- as.factor(df$SectionName)
df$SubsectionName <- as.factor(df$SubsectionName)
df2 <- df
#turn date into date data type
df2$PubDate <- as.POSIXct(strptime(df2$PubDate, "%Y-%m-%d %H:%M:%S"))
str(df2)
df2$DayWeek <- weekdays(df2$PubDate)
df2$DayWeek <- as.factor(df2$DayWeek)
df2$Hour <- format(df2$PubDate, "%H")
df2$Hour <- as.numeric(df2$Hour)
df2$Hour[df2$Hour==0] <- 24
df2$Hour <- as.factor(df2$Hour)
df2$HeadlineCount <- sapply(strsplit(df2$Headline, " "), length)
df2$SnippetCount <- sapply(strsplit(df2$Snippet, " "), length)
df2$AbstractCount <- sapply(strsplit(df2$Abstract, " "), length)
#Create Corpus
library(tm)
library(SnowballC)
# corpusHeadline <- Corpus(VectorSource(df2$Headline))
# corpusHeadline <- tm_map(corpusHeadline, tolower)
# corpusHeadline = tm_map(corpusHeadline, PlainTextDocument)
# corpusHeadline <- tm_map(corpusHeadline, removePunctuation)
# corpusHeadline <- tm_map(corpusHeadline, removeWords, stopwords("english"))
# corpusHeadline <- tm_map(corpusHeadline, stemDocument)
#
# #Convert corpus headline to dtm
# dtmHeadline <- DocumentTermMatrix(corpusHeadline)
#
# #Remove Sparse Words, at least 1%
# dtmHeadline <- removeSparseTerms(dtmHeadline, .992)
#
# #Convert dtm to data frame
# dfHeadline <- as.data.frame(as.matrix(dtmHeadline))
#
# #Some column names (words) are same in both dataframes, so we'll prepend letters to all
# colnames(dfHeadline) = paste0("H", colnames(dfHeadline))
#
#
#
####SNIPPET
# corpusSnip <- Corpus(VectorSource(df2$Snippet))
# corpusSnip <- tm_map(corpusSnip, tolower)
# corpusSnip = tm_map(corpusSnip, PlainTextDocument)
# corpusSnip <- tm_map(corpusSnip, removePunctuation)
# corpusSnip <- tm_map(corpusSnip, removeWords, stopwords("english"))
# corpusSnip <- tm_map(corpusSnip, stemDocument)
#
# #Convert corpus headline to dtm
# dtmSnip <- DocumentTermMatrix(corpusSnip)
#
# #Remove Sparse Words, at least 1%
# dtmSnip <- removeSparseTerms(dtmSnip, .998)
#
# #Convert dtm to data frame
# dfSnip <- as.data.frame(as.matrix(dtmSnip))
#
# #Some column names (words) are same in both dataframes, so we'll prepend letters to all
# colnames(dfSnip) = paste0("S", colnames(dfSnip))
#
#
#
###ABSTRACT
# corpusAbs <- Corpus(VectorSource(df2$Abstract))
# corpusAbs <- tm_map(corpusAbs, tolower)
# corpusAbs = tm_map(corpusAbs, PlainTextDocument)
# corpusAbs <- tm_map(corpusAbs, removePunctuation)
# corpusAbs <- tm_map(corpusAbs, removeWords, stopwords("english"))
# corpusAbs <- tm_map(corpusAbs, stemDocument)
#
# #Convert corpus headline to dtm
# dtmAbs <- DocumentTermMatrix(corpusAbs)
#
# #Remove Sparse Words, at least 1%
# dtmAbs <- removeSparseTerms(dtmAbs, .97
#
# #Convert dtm to data frame
# dfAbs <- as.data.frame(as.matrix(dtmAbs))
#
# #Some column names (words) are same in both dataframes, so we'll prepend letters to all
# colnames(dfAbs) = paste0("A", colnames(dfAbs))
#distances of each data point from the next
# distances <- dist(dfSnip, method="euclidean")
#
# #build hierarchal clustering model
# ModelClust <- hclust(distances, method = "ward.D")
#
# #The distance computation can take a long time if you have a lot of observations and/or
# #if there are a lot of variables.
# #As we saw in recitation, it might not even work if you have too many of either!
#
#
# #look at dendrogram of hclust model
# plot(ModelClust)
#
# #cutoff at 7 clusters, create vector of clusterID to each obsevation (news article).
# clustGroups <- cutree(ModelClust, k=12)
# table(clustGroups)
#
#
# #create a data frame for each cluster, that splits up rows by cluster.
# kosC1 <- dfSnip[clustGroups==1,]
# kosC2 <- dfSnip[clustGroups==2,]
# kosC3 <- dfSnip[clustGroups==3,]
# kosC4 <- dfSnip[clustGroups==4,]
# kosC5 <- dfSnip[clustGroups==5,]
# kosC6 <- dfSnip[clustGroups==6,]
# kosC7 <- dfSnip[clustGroups==7,]
# kosC8 <- dfSnip[clustGroups==8,]
# kosC9 <- dfSnip[clustGroups==9,]
# kosC10 <- dfSnip[clustGroups==10,]
# kosC11 <- dfSnip[clustGroups==11,]
# kosC12 <- dfSnip[clustGroups==12,]
# kosC13 <- dfSnip[clustGroups==13,]
# kosC14 <- dfSnip[clustGroups==14,]
# kosC15 <- dfSnip[clustGroups==15,]
# kosC16 <- dfSnip[clustGroups==16,]
#
#
#What are top 6 most frequent words (columns) in cluster 1?
# tail(sort(colMeans(kosC1)))
# #mean of each column = frequency word ocurs. Sort by order of frequency. Tail = bottom 6
# #but since ordered in ascending, bottom 6 = most frequent 6 words.
#
# tail(sort(colMeans(kosC2)))
# tail(sort(colMeans(kosC3)))
# tail(sort(colMeans(kosC4)))
# tail(sort(colMeans(kosC5)))
# tail(sort(colMeans(kosC6)))
# tail(sort(colMeans(kosC7)))
#
#combine the data frames
#df3 <- cbind(dfHeadline, dfSnip)
#df3 <- cbind(dfHeadline, dfSnip, dfAbs)
df3 <- df2
#df3$Cluster <- clustGroups
df3 <- df3[,c(1, 2, 3, 7, 8, 10, 11, 12, 13, 14)]
# df3$SubsectionName <- as.character(df3$SubsectionName)
# df3$SubsectionName[df3$SectionName == "Home"] <- df3$Cluster[df3$SectionName == "Home"]
# df3$SubsectionName <- as.factor(df3$SubsectionName)
df3 <- df3[,c(2, 3, 4, 5, 6, 7, 8, 9)]
#bring variables into bag of words data frame
#df3$NewsDesk <- df2$NewsDesk
# df3$SectionName <- df2$SectionName
# df3$SubsectionName <- df2$SubsectionName
# df3$WordCount <- df2$WordCount
# df3$DayWeek <- df2$DayWeek
# df3$Hour <- df2$Hour
# df3$HeadlineCount <- df2$HeadlineCount
# df3$SnippetCount <- df2$SnippetCount
# df3$AbstractCount <- df2$AbstractCount
colnames(df3)
Train1 <- df3[1:3500,]
Train2 <- df3[5300:6532,]
Test <- df3[6533:8402,]
Train1$Popular <- trainPopular[1:3500]
Train2$Popular <- trainPopular[5300:6532]
Train <- rbind(Train1, Train2)
#####MODELLING
#Baseline Prediction  --> need to beat 83.27%
table(Train$Popular)
5439/(5439+1093)
#RandomForest, all vars, sparse = .99
library(randomForest)
model1 <- randomForest(Popular ~ . , data = Train)
pred <- predict(model1, newdata = Test, type ="prob")
pred2 <- pred[,2]
ans <- pred2
table(ans>0.5)
predTrain <- predict(model1, type ="prob")
predT <- predTrain[,2]
table(predT>0.5)
table(Train$Popular, predT>0.5)
colnames(Train)
#####SUBMITTING TO KAGGLE
MySubmission2 = data.frame(UniqueID = dftest$UniqueID, Probability1 = ans)
write.csv(MySubmission2, "SubmissionHeadlineLog29.csv", row.names=FALSE)
table(Train$Popular, predT>0.5)
library(mass)
library(MASS)
?shuttle
y <- shuttle
View(y)
View(y)
model1 <- glm(use ~ ., data = y, family = "binomial")
str(model1)
str(y)
model1 <- glm(use ~ wind, data = y, family = "binomial")
str(model1)
summary(model1)
model1 <- glm(use ~ wind + wind:magn, data = y, family = "binomial")
summary(model1)
model1 <- glm(use ~ wind:magn, data = y, family = "binomial")
model1 <- glm(use ~ wind + magn, data = y, family = "binomial")
summary(model1)
model2 <- glm((1 - use) ~ wind + magn, data = y, family = "binomial")
model2 <- glm(1 - use ~ wind + magn, data = y, family = "binomial")
y$use <- as.numeric(y$use)
model2 <- glm(1 - use ~ wind + magn, data = y, family = "binomial")
model2 <- glm(1 - use ~ wind, data = y, family = "binomial")
y$use <- as.factor(y$use)
model2 <- glm(1 - use ~ wind, data = y, family = "binomial")
model2 <- glm(1 - use ~ factor(wind), data = y, family = "binomial")
str(y)
model1 <- glm(use ~ wind + magn, data = y, family = "binomial")
summary(model1)
exp(modl1$coeff)
exp(model1$coeff)
summary(model1)
exp(-.03201)
exp(-0.031)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
View(predictors)
View(predictors)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
trainIndex = createDataPartition(diagnosis, p = 0.50, list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
data(concrete)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
training$ind <- 1:774
View(training)
View(training)
library(ggplot2)
qplot(ind, CompressiveStrength, colour=Cement, data = Training)
qplot(ind, CompressiveStrength, colour=Cement, data = training)
qplot(ind, CompressiveStrength, colour=Age, data = training)
qplot(ind, CompressiveStrength, colour=FlyAsh, data = training)
hist(mixtures$Superplasticizer)
hist(log(mixtures$Superplasticizer))
hist(mixtures$Superplasticizer)
log(1)
log(100)
log(0)
log(0.002)
yu <- log(mixtures$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(training)
View(training)
View(training)
View(training)
which.colname <- "IL_13"
which.colname == "IL_13"
which(colname) == "IL_13"
which(colName) == "IL_13"
grep("^IL$", colnames(df) )
grep("^IL$", colnames(training) )
y <- grep("^IL$", colnames(training))
y
y <- grep("^IL", colnames(training))
y
smalltrain <- training[,58:69]
prComp <- prcomp(smalltrain)
prComp
prComp$rotation
prComp$rotation
?preProcess
pre <- preProcess(smalltrain, method="pca", thresh = .90)
pre$rotation
pre <- preProcess(smalltrain, method="pca", thresh = .90, outcome = training$diagnosis)
pre$rotation
smalltrain$diagnosis <- training$diagnosis
smalltrain[13]
big <- smalltrain[-13]
trainPC <- predict(pre, smalltrain[-13])
modelFit <- train(smalltrain$diagnosis ~ ., method="glm", data=trainPC)
model <- glm(diagnosis ~ ., data = smalltrain)
model <- lm(diagnosis ~ ., data = smalltrain)
smalltrain <- training[,58:69]
smalltrain$diagnosis <- training$diagnosis
model <- lm(diagnosis ~ ., data = smalltrain)
summary(model)
model <- glm(diagnosis ~ ., data = smalltrain, family="binomial")
summary(model)
modelFit <- train(diagnosis ~ ., method="glm", data=smalltrain)
predictions <- predict(modelFit, newdata=testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
smalltest <- testing[,58:69]
smalltest$diagnosis <- testing$diagnosis
predictions <- predict(modelFit, newdata=smalltest)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
modelFitPCA <- train(diagnosis ~ ., method="glm", preProcess = "pca", data=smalltrain)
predictions <- predict(modelFitPCA, newdata=smalltest)
C2 <- confusionMatrix(predictions, testing$diagnosis)
print(C2)
modelFitPCA <- train(diagnosis ~ ., method="glm", preProcess = "pca", data=smalltrain, trControl=trainControl(preProcOptions=list(thresh=0.8)))
predictions <- predict(modelFitPCA, newdata=smalltest)
C2 <- confusionMatrix(predictions, testing$diagnosis)
print(C2)
smalltrain <- training[,58:69]
smalltrain$diagnosis <- training$diagnosis
pre <- preProcess(smalltrain[-13], method="pca", thresh = .80, outcome = smalltrain$diagnosis)
pre$rotation
trainPC <- predict(pre, smalltrain[-13])
modelFit <- train(smalltrain$diagnosis ~ ., method="glm", data=trainPC)
smalltest <- testing[,58:69]
smalltest$diagnosis <- testing$diagnosis
tstPC <- predict(pre, smalltest[-13])
confusionMatrix(smalltest$diagnosis, predict(modelFit, tstPC))
library(shinyapps)
shinyapps::deployApp('C:/Users/Mark Account/Documents/Software/Coursera/Data Products/CourseProjectPart1')
train <- read.csv("C:/Users/Mark Account/Documents/Software/Coursera/Machine Learning/pml-training.csv")
test <- read.csv("C:/Users/Mark Account/Documents/Software/Coursera/Machine Learning/pml-testing.csv")
str(train)
train2 <- train[,c(160, 1:159)]
colnames(train2)
View(train)
View(train)
View(test)
View(test)
View(train)
View(train)
summary(train2)
str(train2)
yy <- is.na(train2[,12])
sum(yy)
yy <- is.na(train2[,15])
sum(yy)
yy <- is.na(train2[,20])
sum(yy)
for(x in 1:160){
u <- is.na(train3[,x])
if(sum(u) > 5000){
train4 <- train4[-x]
}
}
train3 <- train2
train4 <- train3
for(x in 1:160){
u <- is.na(train3[,x])
if(sum(u) > 5000){
train4 <- train4[-x]
}
}
View(train4)
View(train4)
sum(is.na(train2$min_roll_belt))
train4 <- train3
for(x in 1:160){
u <- is.na(train3[,x])
b <- sum(u)
ifb > 5000){
train4 <- train4[-x]
}
}
train4 <- train3
for(x in 1:160){
u <- is.na(train3[,x])
b <- sum(u)
if(b > 5000){
train4 <- train4[-x]
}
}
train4 <- train3
coo <- c()
for(x in 1:160){
u <- is.na(train3[,x])
b <- sum(u)
if(b > 5000){
#train4 <- train4[-x]
coo <- c(coo, x)
}
}
train5 <- train4[-coo]
View(train5)
train <- read.csv("C:/Users/Mark Account/Documents/Software/Coursera/Machine Learning/pml-training.csv", na.strings=c("","NA"))
View(train)
View(train)
train2 <- train[,c(160, 1:159)]
yy <- is.na(train2[,20])
sum(yy)
str(train2)
train3 <- train2
train4 <- train3
coo <- c()
for(x in 1:160){
u <- is.na(train3[,x])
b <- sum(u)
if(b > 5000){
#train4 <- train4[-x]
coo <- c(coo, x)
}
}
train5 <- train4[-coo]
sum(is.na(train2$min_roll_belt))
View(train5)
View(train5)
colnames(train5)
train5[,2]
train6 <- train5[-2]
View(train6)
View(train6)
str(train6)
library(caret)
splitVec <- createDataPartition(y=train6$classe, p=.06, list=F)
training <- train6[splitVec]
testing <- train6[!splitVec]
View(train6)
training <- train6[splitVec,]
testing <- train6[-splitVec,]
xn <- sample(1:19622, 19622, replace=F)
xn
hist(xn)
?sample
19622*.7
num <- 19622*.7
xn <- sample(1:19622, 13735, replace=F)
xn <- sample(1:19622, 19622, replace=F)
t <- xn[1:13735]
tt <- xn[13736:19622]
5887+13735
training <- train6[t,]
testing <- train6[tt,]
model1 <- train(classe ~ ., data=training, method="rf", prox=T)
runApp()
library(shiny)
runApp()
setwd('C:/Users/Mark Account/Documents/Software/Coursera/Data Products/CourseProjectPart1')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(optiRum)
?PMT
runApp()
runApp()
runApp()
runApp()
runApp()
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='markrhine',
token='9C00444A15DD713419905F275D412796',
secret='fRELKVBm87fcy2qxD6i38s03vwJqpyr6JzcgpvrH')
library(shinyapps)
shinyapps::deployApp('C:/Users/Mark Account/Documents/Software/Coursera/Data Products/CourseProjectPart1')
deployApp(appName="Project")
shinyapps::deployApp('C:/Users/Mark Account/Documents/Software/Coursera/Data Products/CourseProjectPart1')
deployApp(appName="Data Products Project")
shinyapps::deployApp('C:/Users/Mark Account/Documents/Software/Coursera/Data Products/CourseProjectPart1')
shinyapps::deployApp('C:/Users/Mark Account/Documents/Software/Coursera/Data Products/CourseProjectPart1')
deployApp(appName="DataProject")
runApp()
deployApp(appName="DataProductsProject")
